{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Wisconsin Breast Cancer Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I worked with the Wisconsin Breast Cancer dataset.  A logistic regression model was trained to predict the diagnosis.\n",
    "\n",
    "The following additional experiments were conducted:\n",
    "1. First, the model was built with all features\n",
    "2. Experiment (1) was repeated, including an intercept\n",
    "3. Experiment (1) was repeated, using randomSplit([0.7, 0.3])\n",
    "4. Experiment (2) was repeated, using randomSplit([0.7, 0.3])\n",
    "\n",
    "The results of (1) vs (2) and the results of (3) vs (4) are discussed at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param init\n",
    "infile = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into dataframe\n",
    "df = spark.read.csv(infile, inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields *f1*, *f2*, *f3* combined into a single *features* column using `VectorAssembler`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\", \"f3\"], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the *diagnosis* and *features* fields for modeling. Convert to RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', DenseVector([17.99, 10.38, 122.8])),\n",
       " ('M', DenseVector([20.57, 17.77, 132.9]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "dataRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map label to binary values, then convert to LabeledPoint\n",
    "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
    "                    .map(lambda row: LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [17.99,10.38,122.8]),\n",
       " LabeledPoint(1.0, [20.57,17.77,132.9])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "lp.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data approximately into training (60%) and test (40%) using `seed=314` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 213, 569)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count records in datasets\n",
    "(training.count(), test.count(), lp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6256590509666081, 0.37434094903339193, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training.count()/lp.count(), test.count()/lp.count(), lp.count()/lp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model by computing the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.8732394366197183\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each of the next experiments, accuracy and confusion matrix are computed for the test set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Build the model using all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9624413145539906\n",
      "Confusion Matrix:\n",
      "[[135.   4.]\n",
      " [  4.  70.]]\n"
     ]
    }
   ],
   "source": [
    "# use all of the fields as features\n",
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "# convert to RDD\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "# map label to binary values, then create LabeledPoints\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "\n",
    "# make sure predictions are floats\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: Repeat experiment 1, with an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9671361502347418\n",
      "Confusion Matrix:\n",
      "[[135.   3.]\n",
      " [  4.  71.]]\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(training, intercept=True)\n",
    "\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3: Repeat experiment 1, with randomSplit([0.7, 0.3])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9433962264150944\n",
      "Confusion Matrix:\n",
      "[[98.  6.]\n",
      " [ 3. 52.]]\n"
     ]
    }
   ],
   "source": [
    "# use all of the fields as features\n",
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "# convert to RDD\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "# map label to binary values, then create LabeledPoints\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = lp.randomSplit([0.7, 0.3], seed=314)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "\n",
    "# make sure predictions are floats\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3: Repeat experiment 2, with randomSplit([0.7, 0.3])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.9371069182389937\n",
      "Confusion Matrix:\n",
      "[[99.  8.]\n",
      " [ 2. 50.]]\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[i for i in df.columns if i[0]=='f'], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "dataRdd = scaledData.select(\"diagnosis\",\"scaledFeatures\").rdd.map(tuple)\n",
    "\n",
    "lp = dataRdd.map(lambda row: LabeledPoint(1 if row[0]=='M' else 0, Vectors.dense(row[1])))\n",
    "\n",
    "training, test = lp.randomSplit([0.7, 0.3], seed=314)\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(training, intercept=True)\n",
    "\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPreds_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results:\n",
    "\n",
    "All four models are extremely accurate. Whats interesting to see however is that the intercept model increased the accuracy of the model only when the train/test partition was 60/40, but when it was 70/30, it did not. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
